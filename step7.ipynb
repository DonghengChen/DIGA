{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('router_data/1128_newcon2_2.json','r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收集成功的证明\n",
    "success = []\n",
    "for item in data:\n",
    "    if item['success']['1128_newcon2_2']:\n",
    "        success.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行ij反解\n",
    "news = set()\n",
    "for item in success:\n",
    "    name = item['name']\n",
    "    try:\n",
    "        i = int(name.split('_')[-2])\n",
    "        j = int(name.split('_')[-1])\n",
    "        item['id'] = (i,j)\n",
    "        item['kname'] = '_'.join(name.split('_')[:-2])\n",
    "    except:\n",
    "        item['id'] = (-1,-1)\n",
    "        item['kname'] = name\n",
    "    news.add(item['kname'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limengze/miniconda3/envs/ATP_Train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/limengze/miniconda3/envs/ATP_Train/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/limengze/miniconda3/envs/ATP_Train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/limengze/miniconda3/envs/ATP_Train/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/limengze/miniconda3/envs/ATP_Train/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.74s/it]\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4,5\"\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "# 选择预训练模型，例如BERT\n",
    "model_name = ['deepseek-ai/DeepSeek-Prover-V1.5-RL','/home/limengze/DeepSeek-Coder/finetune/finetune_result5/checkpoint-40']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name[0])\n",
    "model = AutoModel.from_pretrained(model_name[0],device_map='auto')\n",
    "model.config.output_hidden_states = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1127con/aime_amc_new_con2.jsonl','r') as f:\n",
    "    raw = f.readlines()\n",
    "    raw = [json.loads(item) for item in raw]\n",
    "# 读取原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     seg_map \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm(seg_map):\n\u001b[1;32m      7\u001b[0m         emb_list\u001b[38;5;241m=\u001b[39m[]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# 拿到所有subgoals的嵌入\n",
    "with open('1127con/seg_map.json','r') as f:\n",
    "    seg_map = json.load(f)\n",
    "from tqdm import tqdm\n",
    "with torch.no_grad():\n",
    "    for item in tqdm(seg_map):\n",
    "        emb_list=[]\n",
    "        if type(seg_map[item])== str:\n",
    "            tokens=tokenizer(text, return_tensors='pt')\n",
    "            output = model(**tokens)\n",
    "            layers = []\n",
    "            # 保存输出\n",
    "            for i in range(len(output.hidden_states)):\n",
    "                embeddings = output.hidden_states[i]\n",
    "                embeddings = embeddings.numpy().mean(axis=1)\n",
    "                layers.append(embeddings)\n",
    "            emb_list.append(layers)\n",
    "            text0 = seg_map[item]\n",
    "        else:\n",
    "            for text in seg_map[item]:\n",
    "                tokens=tokenizer(text, return_tensors='pt')\n",
    "                output = model(**tokens)\n",
    "                layers = []\n",
    "                # 保存输出\n",
    "                for i in range(len(output.hidden_states)):\n",
    "                    embeddings = output.hidden_states[i]\n",
    "                    embeddings = embeddings.numpy().mean(axis=1)\n",
    "                    layers.append(embeddings)\n",
    "                emb_list.append(layers)\n",
    "            text0 = seg_map[item].copy()\n",
    "        seg_map[item] = {'emb':emb_list,'text':text0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Certainly! Let's fill in the details of each step in the outlined proof.\\n\\n**Problem Explanation:**\\nSusie pays for \\\\(4\\\\) muffins and \\\\(3\\\\) bananas, and Calvin spends twice as much on \\\\(2\\\\) muffins and \\\\(16\\\\) bananas. We need to determine how many times more expensive a muffin is compared to a banana.\\n\\n**Define Variables:**\\nLet \\\\( m \\\\) be the cost of one muffin and \\\\( b \\\\) be the cost of one banana.\\n\\n**1. Susie's and Calvin's Total Costs:**\\n- Susie's cost: \\\\( 4m + 3b \\\\)\\n- Calvin's cost: \\\\( 2m + 16b \\\\)\\n- Given: \\\\( \\\\text{Calvin's cost} = 2 \\\\times \\\\text{Susie's cost} \\\\)\\n\\nThis leads to the equation:\\n\\\\[ 2m + 16b = 2(4m + 3b) \\\\]\\n\\n**2. Simplification and Rearrangement:**\\nSimplify the equation:\\n\\\\[ 2m + 16b = 8m + 6b \\\\]\\n\\nRearrange by moving all terms involving \\\\( m \\\\) to one side and terms involving \\\\( b \\\\) to the other side:\\n\\\\[ 2m - 8m + 16b - 6b = 0 \\\\]\\n\\nCombine like terms:\\n\\\\[ -6m + 10b = 0 \\\\]\\n\\n**3. Express \\\\( m \\\\) in terms of \\\\( b \\\\):**\\nWe rearrange the equation to express \\\\( m \\\\) in terms of \\\\( b \\\\):\\n\\\\[ 10b = 6m \\\\]\\n\\n**4. Solve for the ratio \\\\( \\\\frac{m}{b} \\\\):**\\nDivide both sides of the equation by \\\\( 6b \\\\) to isolate \\\\( \\\\frac{m}{b} \\\\):\\n\\\\[ \\\\frac{m}{b} = \\\\frac{10}{6} \\\\]\\n\\nSimplify the fraction by dividing the numerator and the denominator by their greatest common divisor, which is \\\\( 2 \\\\):\\n\\\\[ \\\\frac{m}{b} = \\\\frac{5}{3} \\\\]\\n\\n**Conclusion:**\\nA muffin is \\\\(\\\\frac{5}{3}\\\\) times as expensive as a banana. \\n\\nThe answer is \\\\(\\\\boxed{\\\\frac{5}{3}}\\\\).\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('1127con/seg_map_emb.pkl','wb') as f:\n",
    "#     pickle.dump(seg_map,f)\n",
    "# 读取权重\n",
    "with open('1127con/seg_map_emb.pkl','rb') as f:\n",
    "    seg_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在news阶段就要划分数据集\n",
    "import random\n",
    "random.seed(0)\n",
    "news = list(news)\n",
    "random.shuffle(news)\n",
    "# 训练集\n",
    "train = news[:int(len(news)*0.5)]\n",
    "# 测试集\n",
    "test = news[int(len(news)*0.5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建良性数据集\n",
    "good_vectors = {}\n",
    "bad_vectors = {}\n",
    "for item in success:\n",
    "    key = item['kname']\n",
    "    if item['id'] == (-1,-1):\n",
    "        continue # 不与采用\n",
    "        print(len(seg_map[item['name']]['emb']))\n",
    "    else:\n",
    "        i,j = item['id']\n",
    "        if key in good_vectors.keys():\n",
    "            good_vectors[key].add(i)\n",
    "            good_vectors[key].add(j)\n",
    "        else:\n",
    "            good_vectors[key] = set([i,j])\n",
    "\n",
    "for item in list(news):\n",
    "    d=len(seg_map[item]['emb'])\n",
    "    bad_vectors[item] = set(range(d))\n",
    "for item in good_vectors:\n",
    "    bad_vectors[item] = bad_vectors[item] - good_vectors[item]\n",
    "\n",
    "# 清除bad_vectors中的空集\n",
    "bad_vectors = {k:v for k,v in bad_vectors.items() if len(v)>0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark 中的向量都是训练集的\n",
    "mark=[]\n",
    "for item in train:\n",
    "    if item in good_vectors.keys():\n",
    "        for i in good_vectors[item]:\n",
    "            mark.append((seg_map[item]['emb'][i],1))\n",
    "for item in train:\n",
    "    if item in bad_vectors.keys():\n",
    "        for i in bad_vectors[item]:\n",
    "            mark.append((seg_map[item]['emb'][i],-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义最近邻分类\n",
    "def dist(x,y):\n",
    "    layer = 2\n",
    "    vec1 = x[layer]\n",
    "    vec2 = y[layer]\n",
    "    return ((vec1-vec2)**2).sum()**0.5\n",
    "\n",
    "\n",
    "def classify(x,Y):\n",
    "    dists =  [dist(x,y[0]) for y in Y]\n",
    "    one=0\n",
    "    for i in range(len(dists)):\n",
    "        one += Y[i][1]/dists[i]\n",
    "    one = one/len(dists)\n",
    "    return one\n",
    "\n",
    "def Test(emb,ans,Y):\n",
    "    ones = [(classify(emb[i],Y),i) for i in range(len(emb))]\n",
    "    # 对ones进行排序,根据key\n",
    "    ones = sorted(ones,key=lambda x:-x[0])\n",
    "    print(ones)\n",
    "    print(ans)\n",
    "    if ones[0][1] in ans and ones[1][1] in ans:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 2, 4, 5}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seg_map[test[0]] embeding\n",
    "#  good_vectors[test[0]] answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.float32(0.023542628), 4), (np.float32(0.021882748), 0), (np.float32(0.02145388), 5), (np.float32(0.0164782), 2), (np.float32(0.015883036), 1), (np.float32(0.015849581), 3), (np.float32(0.014806216), 6)]\n",
      "{0, 2, 4, 5}\n",
      "[(np.float32(0.039501518), 2), (np.float32(0.030569986), 1), (np.float32(0.019862587), 0), (np.float32(0.015402619), 3), (np.float32(0.014718563), 4)]\n",
      "{1, 2, 3, 4}\n",
      "[(np.float32(0.03385408), 2), (np.float32(0.032268986), 6), (np.float32(0.031620406), 0), (np.float32(0.02542017), 4), (np.float32(0.018254077), 5), (np.float32(0.017550088), 3), (np.float32(0.011805613), 1)]\n",
      "{0, 1, 2, 3, 4, 5, 6}\n",
      "[(np.float32(0.0363795), 2), (np.float32(0.02066771), 1), (np.float32(0.009717819), 0)]\n",
      "{0, 1, 2}\n",
      "[(np.float32(0.022958659), 3), (np.float32(0.015790846), 2), (np.float32(0.015344508), 1), (np.float32(0.0142891295), 0)]\n",
      "{0, 1}\n",
      "[(np.float32(0.030891642), 6), (np.float32(0.030462552), 5), (np.float32(0.026765745), 4), (np.float32(0.026349768), 1), (np.float32(0.025482452), 2), (np.float32(0.024355434), 0), (np.float32(0.02080248), 7), (np.float32(0.01756909), 8), (np.float32(0.012147992), 3)]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n",
      "[(np.float32(0.03742467), 2), (np.float32(0.035310093), 1), (np.float32(0.028348112), 5), (np.float32(0.027544696), 4), (np.float32(0.021750452), 0), (np.float32(0.021482354), 3)]\n",
      "{0, 1, 2, 4, 5}\n",
      "[(np.float32(0.030635962), 3), (np.float32(0.022822548), 0), (np.float32(0.013621144), 4), (np.float32(0.010748204), 2), (np.float32(0.00979245), 1)]\n",
      "{0, 1, 2, 3, 4}\n",
      "[(np.float32(0.03458254), 1), (np.float32(0.031435337), 2), (np.float32(0.031019542), 0), (np.float32(0.0216173), 4), (np.float32(0.019831553), 3)]\n",
      "{0, 1, 2, 3, 4}\n",
      "[(np.float32(0.042339455), 0), (np.float32(0.02581944), 3), (np.float32(0.024231287), 2), (np.float32(0.023423813), 1), (np.float32(0.0049795313), 4)]\n",
      "{1, 2, 3, 4}\n",
      "[(np.float32(0.031937085), 2), (np.float32(0.025507834), 4), (np.float32(0.02170062), 3), (np.float32(0.02078885), 1), (np.float32(0.020460714), 0), (np.float32(0.020283021), 6), (np.float32(0.01477975), 5)]\n",
      "{0, 1, 2, 3, 4, 5, 6}\n",
      "[(np.float32(0.03504569), 3), (np.float32(0.034825373), 4), (np.float32(0.03310696), 2), (np.float32(0.028278844), 5), (np.float32(0.020443417), 1), (np.float32(0.019913785), 0)]\n",
      "{0, 1, 2, 3, 4, 5}\n",
      "[(np.float32(0.016477892), 4), (np.float32(0.013725472), 3), (np.float32(0.011238559), 0), (np.float32(0.008332409), 1), (np.float32(0.0038575334), 2)]\n",
      "{0, 3, 4}\n",
      "[(np.float32(0.030782608), 8), (np.float32(0.030302476), 4), (np.float32(0.029531583), 2), (np.float32(0.027544146), 9), (np.float32(0.026623132), 10), (np.float32(0.026085446), 6), (np.float32(0.025643032), 12), (np.float32(0.024809843), 11), (np.float32(0.024640393), 1), (np.float32(0.023317441), 13), (np.float32(0.022283258), 5), (np.float32(0.022248412), 7), (np.float32(0.02224329), 3), (np.float32(0.019239385), 0), (np.float32(0.0013474019), 14)]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}\n",
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "# 统计正确率\n",
    "total = 0\n",
    "correct = 0\n",
    "for item in test:\n",
    "    try:\n",
    "        correct+=Test(seg_map[item]['emb'],good_vectors[item],mark)\n",
    "        total+=1\n",
    "    except:\n",
    "        pass\n",
    "print(correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATP_Train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
